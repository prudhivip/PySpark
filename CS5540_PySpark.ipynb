{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fah8471C0Xrs"
      },
      "source": [
        "## Import & Install PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqcUxUVy0cm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e57bd7c-c40f-4ca6-9a0d-00f89a7bb62b"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=dc71a1e0e2dfe742311852689eb640464cbd09f15cddae3132c37e2308e47f2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq79Coox0rq-"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "sc = pyspark.SparkContext('local[*]')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52jjSxfR6nrx"
      },
      "source": [
        "### pyspark.sql.SparkSession Main entry point for DataFrame and SQL functionality.\n",
        "#### The entry point to programming Spark with the Dataset and DataFrame API.\n",
        "\n",
        "A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:\n",
        "[Source](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pqKAw136bQi"
      },
      "source": [
        "spark = SparkSession.builder.master(\"local\").appName(\"cs5540\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vat_m6tr1vx4"
      },
      "source": [
        "# We can create RDD in two ways:\n",
        "- parallelize()\n",
        "\n",
        "- textFile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbwbwVzH2EwZ"
      },
      "source": [
        "### parallelize()  to create RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwslRUNs1vFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ece68a-562a-4c3f-b47f-bec8c2e11631"
      },
      "source": [
        "big_list = range(1000)\n",
        "print(type(big_list))\n",
        "\n",
        "# make an RDD and distripute the data into 2 partitions:\n",
        "rdd = sc.parallelize(big_list, 2)\n",
        "print(type(rdd))\n",
        "\n",
        "# filter Return a new RDD containing only the elements that satisfy a predicate.\n",
        "# By using the RDD filter() method, that operation occurs in a distributed manner\n",
        "# across several CPUs or computers.\n",
        "\n",
        "# Here we filter the odd numbers\n",
        "odds = rdd.filter(lambda x: x % 2 != 0)\n",
        "print(type(odds))\n",
        "\n",
        "# Here we filter the even numbers\n",
        "evens = rdd.filter(lambda x: x % 2 == 0)\n",
        "print(type(evens))\n",
        "\n",
        "# take() is a way to see the contents of your RDD, but only a small subset, by\n",
        "# pulling that subset of data from the distributed system onto a single machine.\n",
        "print(odds.take(10))\n",
        "print(evens.take(10))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'range'>\n",
            "<class 'pyspark.rdd.PipelinedRDD'>\n",
            "<class 'pyspark.rdd.PipelinedRDD'>\n",
            "<class 'pyspark.rdd.PipelinedRDD'>\n",
            "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYV7bf_Z89OK"
      },
      "source": [
        "# textFile() to create RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_RncShx9BFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201a2f18-ed24-4fdd-9183-039b3b657586"
      },
      "source": [
        "txt = sc.textFile('/content/links.txt')\n",
        "print(type(txt))\n",
        "\n",
        "# Print the number of lines counted\n",
        "print(txt.count())\n",
        "\n",
        "# filter lines with word \"python\"\n",
        "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
        "\n",
        "# count the number of lines with word \"python\"\n",
        "print(python_lines.count())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "28\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E731ZM2R9sB0"
      },
      "source": [
        "## Lazy Evaluation\n",
        "Results are not computed right away. Instead, they just remember the transformations applied to some base data set.Spark computes transformations when an action requires a result for the driver program.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKGVCQB_-ldF"
      },
      "source": [
        "## Reading a Tweet JSON File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTyjSWeQ9aXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97923887-d80c-4f87-c024-c085ea0055f4"
      },
      "source": [
        "path_to_input = '/content/tweet.json'\n",
        "df = spark.read.json(sc.wholeTextFiles(path_to_input).values())\n",
        "\n",
        "print(type(df))\n",
        "df.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "+------------+-----------+--------------------+--------------------+---------+----+-----------------+-----------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+-----+-------------+---------+--------------------+--------------------+--------------------+---------+--------------------+\n",
            "|contributors|coordinates|          created_at|            entities|favorited| geo|               id|           id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|place|retweet_count|retweeted|    retweeted_status|              source|                text|truncated|                user|\n",
            "+------------+-----------+--------------------+--------------------+---------+----+-----------------+-----------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+-----+-------------+---------+--------------------+--------------------+--------------------+---------+--------------------+\n",
            "|        null|       null|Sun Apr 03 23:48:...|{[], [], [{271572...|    false|null|54691802283900930|54691802283900928|                   null|                 null|                     null|               null|                   null| null|            4|    false|{null, null, Sun ...|<a href=\"http://t...|RT @PostGradProbl...|     true|{false, Tue Oct 0...|\n",
            "+------------+-----------+--------------------+--------------------+---------+----+-----------------+-----------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+-----+-------------+---------+--------------------+--------------------+--------------------+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh29-qqq_Inp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa2451d-3b50-450d-c918-65f4cde1de31"
      },
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createOrReplaceTempView(\"tweet\")\n",
        "\n",
        "q1 = spark.sql('select user.screen_name, user.followers_count, text, retweet_count from tweet')\n",
        "# Collect (Action) - Return all the elements of the dataset as an array at the driver program.\n",
        "print(q1.collect())\n",
        "print(q1.show())\n",
        "\n",
        "\n",
        "q2 = spark.sql('select created_at, source from tweet')\n",
        "# Collect (Action) - Return all the elements of the dataset as an array at the driver program.\n",
        "print(q2.collect())\n",
        "print(q2.show())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(screen_name='OldGREG85', followers_count=48, text='RT @PostGradProblem: In preparation for the NFL lockout, I will be spending twice as much time analyzing my fantasy baseball team during ...', retweet_count=4)]\n",
            "+-----------+---------------+--------------------+-------------+\n",
            "|screen_name|followers_count|                text|retweet_count|\n",
            "+-----------+---------------+--------------------+-------------+\n",
            "|  OldGREG85|             48|RT @PostGradProbl...|            4|\n",
            "+-----------+---------------+--------------------+-------------+\n",
            "\n",
            "None\n",
            "[Row(created_at='Sun Apr 03 23:48:36 +0000 2011', source='<a href=\"http://twitter.com/\" rel=\"nofollow\">Twitter for iPhone</a>')]\n",
            "+--------------------+--------------------+\n",
            "|          created_at|              source|\n",
            "+--------------------+--------------------+\n",
            "|Sun Apr 03 23:48:...|<a href=\"http://t...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAAjfkWo_LQW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmtZywUGfthO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}